{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6b69081-44b3-4068-85b1-7665089a5cd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Configuring Auto Loader for Reliable Ingestion\n",
    "\n",
    "When using Auto Loader, you can configure several options for your stream to ensure reliable data ingestion:\n",
    "\n",
    "\n",
    "\n",
    "### Setting Maximum Bytes per Trigger\n",
    "\n",
    "If you're ingesting large files that cause long micro-batch processing times or memory issues, you can use the __cloudFiles.maxBytesPerTrigger__ option to control the maximum amount of data processed in each micro-batch. This improves stability and keeps batch durations more predictable. For example, to limit each micro-batch to 1 GB of data, you can configure your stream as follows:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9aab771-85b5-4492-aa39-a5a289a212ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.readStream\n",
    "     .format(\"cloudFiles\")\n",
    "     .option(\"cloudFiles.format\", <source_format>)\n",
    "     .option(\"cloudFiles.maxBytesPerTrigger\", \"1g\")\n",
    "     .load(\"/path/to/files\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22e8a45c-3d00-4234-8391-ae0f0ce34cde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Handling Bad Records\n",
    "\n",
    "When working with JSON or CSV files, you can use the badRecordsPath option to capture and isolate invalid records in a separate location for further review. Records with malformed syntax (e.g., missing brackets, extra commas) or schema mismatches (e.g., data type errors, missing fields) are redirected to the specified path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8444860d-4865-4330-a5a2-fe3362ca9720",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.readStream\n",
    "     .format(\"cloudFiles\")\n",
    "     .option(\"cloudFiles.format\", \"json\")\n",
    "     .option(\"badRecordsPath\", \"/path/to/quarantine\")\n",
    "     .schema(\"id int, value double\")\n",
    "     .load(\"/path/to/files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87ae5f13-6616-4111-b1c2-723f3350e623",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Files Filters:\n",
    "\n",
    "To filter input files based on a specific pattern, such as *.png, you can use the pathGlobFilter option. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c937e2cd-2cb0-4e35-b753-a14470facb3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.readStream\n",
    "     .format(\"cloudFiles\")\n",
    "     .option(\"cloudFiles.format\", \"binaryFile\")\n",
    "     .option(\"pathGlobfilter\", \"*.png\")\n",
    "     .load(\"/path/to/files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84e6d574-4c96-4429-bcd7-6325dcafe6cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Schema Evolution:\n",
    "\n",
    "Auto Loader detects the addition of new columns in input files during processing. To control how this schema change is handled, you can set cloudFiles.schemaEvolutionMode option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74f8eea9-69e5-4b5b-8a66-a46951322178",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.readStream\n",
    "     .format(\"cloudFiles\")\n",
    "     .option(\"cloudFiles.format\", <source_format>)\n",
    "     .option(\"cloudFiles.schemaEvolutionMode\", <mode>)\n",
    "     .load(\"/path/to/files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6f6076b-eeee-4324-828c-066172871984",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### The supported schema evolution modes include:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b08693d3-c3cf-413e-a20c-06002877dc6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "__addNewColumns__ (default) --> Stream fails. New columns are added to the schema.\n",
    "\n",
    "__rescue__ --> Schema is never evolved and stream does not fail due to schema changes. All new columns are recorded in the resued data column.\n",
    "\n",
    "__failOnNewColumns__ --> Stream fails. Stream does not restart unless the provided schema is updated or the offending file is removed.\n",
    "\n",
    "__none__ --> Does not evolve the schema, new columns are ignored, and data is not rescued. Stream does not fail due to schema changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41c4d391-9a60-47d2-9884-ed42770ed5c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The default mode is addNewColumns, so when Auto Loader detects a new column, the stream stops with an UnknownFieldException. Before your stream throws this error, Auto Loader updates the schema location with the latest schema by merging new columns to the end of the schema. The next run of the stream executes successfully with the updated schema.\n",
    "\n",
    "Note that the addNewColumns mode is the default when a schema is not provided, but none is the default when you provide a schema. addNewColumns is not allowed when the schema of the stream is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63690e6c-2245-4bcc-abcc-38bb87d9e8ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Configuring Auto Loader for Reliable Ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
